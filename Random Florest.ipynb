{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68d44d03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-02 12:10:17,941 WARN util.Utils: Your hostname, BDS-2023 resolves to a loopback address: 127.0.1.1; using 10.0.2.15 instead (on interface enp0s3)\n",
      "2024-04-02 12:10:17,949 WARN util.Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "2024-04-02 12:11:05,863 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /__ / .__/\\_,_/_/ /_/\\_\\   version 3.2.3\n",
      "      /_/\n",
      "\n",
      "Using Python version 3.10.12 (main, Nov 20 2023 15:14:05)\n",
      "Spark context Web UI available at http://10.0.2.15:4040\n",
      "Spark context available as 'sc' (master = local[*], app id = local-1712056298947).\n",
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.shell import sqlContext\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e90efc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession, SQLContext\n",
    "#data input\n",
    "\n",
    "spark_yog = SparkSession.builder.appName('RandomFlorest').getOrCreate()\n",
    "df_yog = spark_yog.read.csv('hdfs://localhost:9000/user/hduser/Data_Combined.csv', header = True, inferSchema = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a1d8028",
   "metadata": {},
   "outputs": [],
   "source": [
    "# processing data\n",
    "\n",
    "cloum_set=df_yog.columns\n",
    "categorical_Columns = ['proto', 'service', 'state']\n",
    "stages_feat = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "720b01f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# changes the data into vectors that library can undestand and saved in stages_feat\n",
    "\n",
    "for categorical_Col in categorical_Columns:\n",
    "    stringIndexer = StringIndexer(inputCol = categorical_Col, outputCol = categorical_Col + 'Index')\n",
    "    encoder = OneHotEncoder(inputCols=[stringIndexer.getOutputCol()], outputCols=[categorical_Col + \"classVec\"])\n",
    "    stages_feat += [stringIndexer, encoder]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "340b2458",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change/Feed Label\n",
    "Label_str_Indexer = StringIndexer(inputCol = 'attack_cat', outputCol = 'Pred_Label')\n",
    "stages_feat += [Label_str_Indexer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "908f949d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#all labels used\n",
    "numeric_Cols=['dur','spkts','dpkts','sbytes','dbytes','rate','sttl','dttl','sload','dload','sloss','dloss','sinpkt','dinpkt','sjit','djit','swin','stcpb','dtcpb','dwin','tcprtt','synack','ackdat','smean','dmean','trans_depth','response_body_len','ct_srv_src','ct_state_ttl'\t,'ct_dst_ltm',\t'ct_src_dport_ltm','ct_dst_sport_ltm','ct_dst_src_ltm','is_ftp_login','ct_ftp_cmd','ct_flw_http_mthd','ct_src_ltm','ct_srv_dst','is_sm_ips_ports']\n",
    "# Random 5 labels\n",
    "# numeric_Cols= ['sttl', 'sbytes', 'dwin', 'smean', 'dmean']\n",
    "# Another set of random 5 labels\n",
    "# numeric_Cols = ['sloss', 'sload', 'stcpb', 'dtcpb', 'trans_depth']\n",
    "assembler_Inputs = [c + \"classVec\" for c in categorical_Columns] + numeric_Cols\n",
    "#labeling the output\n",
    "Assembler = VectorAssembler(inputCols=assembler_Inputs, outputCol=\"features_all\")\n",
    "stages_feat += [Assembler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "77d8b866",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Pipeline processing of data to take into the library\n",
    "pipeline = Pipeline(stages = stages_feat)\n",
    "pipelineModel = pipeline.fit(df_yog)\n",
    "df_yog = pipelineModel.transform(df_yog)\n",
    "selected_Cols = ['Pred_Label', 'features_all'] + cloum_set\n",
    "df_yog = df_yog.select(selected_Cols)\n",
    "# df_yog.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b3f84bff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-02 12:22:56,425 WARN util.package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Dataset Count: 112371\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 19:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Dataset Count: 52293\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# splits = df_yog.randomSplit([0.6,0.4], 1234)\n",
    "training_data, testing_data = df_yog.randomSplit([0.6805,0.3195], seed = 99999999)\n",
    "print(\"Training Dataset Count: \" + str(training_data.count()))\n",
    "print(\"Test Dataset Count: \" + str(testing_data.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6aaa5c88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#Model\n",
    "RandomForest = RandomForestClassifier(labelCol=\"Pred_Label\", featuresCol=\"features_all\", numTrees=10)\n",
    "\n",
    "start=time.time()\n",
    "RandomForestModel = RandomForest.fit(training_data)\n",
    "end=time.time()\n",
    "start1=time.time()\n",
    "f_predictions = RandomForestModel.transform(testing_data)\n",
    "end1=time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c8010e79",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "cannot resolve 'PoutLabel' given input columns: [Pred_Label, ackdat, attack_cat, ct_dst_ltm, ct_dst_sport_ltm, ct_dst_src_ltm, ct_flw_http_mthd, ct_ftp_cmd, ct_src_dport_ltm, ct_src_ltm, ct_srv_dst, ct_srv_src, ct_state_ttl, dbytes, dinpkt, djit, dload, dloss, dmean, dpkts, dtcpb, dttl, dur, dwin, features_all, id, is_ftp_login, is_sm_ips_ports, label, prediction, probability, proto, rate, rawPrediction, response_body_len, sbytes, service, sinpkt, sjit, sload, sloss, smean, spkts, state, stcpb, sttl, swin, synack, tcprtt, trans_depth];\n'Aggregate ['PoutLabel, prediction#1920], ['PoutLabel, prediction#1920, count(1) AS count#2075L]\n+- Project [Pred_Label#1019, features_all#1104, id#16, dur#17, proto#18, service#19, state#20, spkts#21, dpkts#22, sbytes#23, dbytes#24, rate#25, sttl#26, dttl#27, sload#28, dload#29, sloss#30, dloss#31, sinpkt#32, dinpkt#33, sjit#34, djit#35, swin#36, stcpb#37L, ... 26 more fields]\n   +- Project [Pred_Label#1019, features_all#1104, id#16, dur#17, proto#18, service#19, state#20, spkts#21, dpkts#22, sbytes#23, dbytes#24, rate#25, sttl#26, dttl#27, sload#28, dload#29, sloss#30, dloss#31, sinpkt#32, dinpkt#33, sjit#34, djit#35, swin#36, stcpb#37L, ... 25 more fields]\n      +- Project [Pred_Label#1019, features_all#1104, id#16, dur#17, proto#18, service#19, state#20, spkts#21, dpkts#22, sbytes#23, dbytes#24, rate#25, sttl#26, dttl#27, sload#28, dload#29, sloss#30, dloss#31, sinpkt#32, dinpkt#33, sjit#34, djit#35, swin#36, stcpb#37L, ... 24 more fields]\n         +- Sample 0.6805, 1.0, false, 99999999\n            +- Sort [Pred_Label#1019 ASC NULLS FIRST, features_all#1104 ASC NULLS FIRST, id#16 ASC NULLS FIRST, dur#17 ASC NULLS FIRST, proto#18 ASC NULLS FIRST, service#19 ASC NULLS FIRST, state#20 ASC NULLS FIRST, spkts#21 ASC NULLS FIRST, dpkts#22 ASC NULLS FIRST, sbytes#23 ASC NULLS FIRST, dbytes#24 ASC NULLS FIRST, rate#25 ASC NULLS FIRST, sttl#26 ASC NULLS FIRST, dttl#27 ASC NULLS FIRST, sload#28 ASC NULLS FIRST, dload#29 ASC NULLS FIRST, sloss#30 ASC NULLS FIRST, dloss#31 ASC NULLS FIRST, sinpkt#32 ASC NULLS FIRST, dinpkt#33 ASC NULLS FIRST, sjit#34 ASC NULLS FIRST, djit#35 ASC NULLS FIRST, swin#36 ASC NULLS FIRST, stcpb#37L ASC NULLS FIRST, ... 23 more fields], false\n               +- Project [Pred_Label#1019, features_all#1104, id#16, dur#17, proto#18, service#19, state#20, spkts#21, dpkts#22, sbytes#23, dbytes#24, rate#25, sttl#26, dttl#27, sload#28, dload#29, sloss#30, dloss#31, sinpkt#32, dinpkt#33, sjit#34, djit#35, swin#36, stcpb#37L, ... 23 more fields]\n                  +- Project [id#16, dur#17, proto#18, service#19, state#20, spkts#21, dpkts#22, sbytes#23, dbytes#24, rate#25, sttl#26, dttl#27, sload#28, dload#29, sloss#30, dloss#31, sinpkt#32, dinpkt#33, sjit#34, djit#35, swin#36, stcpb#37L, dtcpb#38L, dwin#39, ... 29 more fields]\n                     +- Project [id#16, dur#17, proto#18, service#19, state#20, spkts#21, dpkts#22, sbytes#23, dbytes#24, rate#25, sttl#26, dttl#27, sload#28, dload#29, sloss#30, dloss#31, sinpkt#32, dinpkt#33, sjit#34, djit#35, swin#36, stcpb#37L, dtcpb#38L, dwin#39, ... 28 more fields]\n                        +- Project [id#16, dur#17, proto#18, service#19, state#20, spkts#21, dpkts#22, sbytes#23, dbytes#24, rate#25, sttl#26, dttl#27, sload#28, dload#29, sloss#30, dloss#31, sinpkt#32, dinpkt#33, sjit#34, djit#35, swin#36, stcpb#37L, dtcpb#38L, dwin#39, ... 27 more fields]\n                           +- Project [id#16, dur#17, proto#18, service#19, state#20, spkts#21, dpkts#22, sbytes#23, dbytes#24, rate#25, sttl#26, dttl#27, sload#28, dload#29, sloss#30, dloss#31, sinpkt#32, dinpkt#33, sjit#34, djit#35, swin#36, stcpb#37L, dtcpb#38L, dwin#39, ... 26 more fields]\n                              +- Project [id#16, dur#17, proto#18, service#19, state#20, spkts#21, dpkts#22, sbytes#23, dbytes#24, rate#25, sttl#26, dttl#27, sload#28, dload#29, sloss#30, dloss#31, sinpkt#32, dinpkt#33, sjit#34, djit#35, swin#36, stcpb#37L, dtcpb#38L, dwin#39, ... 25 more fields]\n                                 +- Project [id#16, dur#17, proto#18, service#19, state#20, spkts#21, dpkts#22, sbytes#23, dbytes#24, rate#25, sttl#26, dttl#27, sload#28, dload#29, sloss#30, dloss#31, sinpkt#32, dinpkt#33, sjit#34, djit#35, swin#36, stcpb#37L, dtcpb#38L, dwin#39, ... 24 more fields]\n                                    +- Project [id#16, dur#17, proto#18, service#19, state#20, spkts#21, dpkts#22, sbytes#23, dbytes#24, rate#25, sttl#26, dttl#27, sload#28, dload#29, sloss#30, dloss#31, sinpkt#32, dinpkt#33, sjit#34, djit#35, swin#36, stcpb#37L, dtcpb#38L, dwin#39, ... 23 more fields]\n                                       +- Project [id#16, dur#17, proto#18, service#19, state#20, spkts#21, dpkts#22, sbytes#23, dbytes#24, rate#25, sttl#26, dttl#27, sload#28, dload#29, sloss#30, dloss#31, sinpkt#32, dinpkt#33, sjit#34, djit#35, swin#36, stcpb#37L, dtcpb#38L, dwin#39, ... 22 more fields]\n                                          +- Relation [id#16,dur#17,proto#18,service#19,state#20,spkts#21,dpkts#22,sbytes#23,dbytes#24,rate#25,sttl#26,dttl#27,sload#28,dload#29,sloss#30,dloss#31,sinpkt#32,dinpkt#33,sjit#34,djit#35,swin#36,stcpb#37L,dtcpb#38L,dwin#39,... 21 more fields] csv\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_6937/4152164876.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# #PRINT CONFUSION MATRIX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#Cm=f_predictions.select(\"PoutLabel\",\"label\").distinct().toPandas()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mf_predictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"PoutLabel\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"prediction\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/sql/group.py\u001b[0m in \u001b[0;36m_api\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_api\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jgd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1321\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: cannot resolve 'PoutLabel' given input columns: [Pred_Label, ackdat, attack_cat, ct_dst_ltm, ct_dst_sport_ltm, ct_dst_src_ltm, ct_flw_http_mthd, ct_ftp_cmd, ct_src_dport_ltm, ct_src_ltm, ct_srv_dst, ct_srv_src, ct_state_ttl, dbytes, dinpkt, djit, dload, dloss, dmean, dpkts, dtcpb, dttl, dur, dwin, features_all, id, is_ftp_login, is_sm_ips_ports, label, prediction, probability, proto, rate, rawPrediction, response_body_len, sbytes, service, sinpkt, sjit, sload, sloss, smean, spkts, state, stcpb, sttl, swin, synack, tcprtt, trans_depth];\n'Aggregate ['PoutLabel, prediction#1920], ['PoutLabel, prediction#1920, count(1) AS count#2075L]\n+- Project [Pred_Label#1019, features_all#1104, id#16, dur#17, proto#18, service#19, state#20, spkts#21, dpkts#22, sbytes#23, dbytes#24, rate#25, sttl#26, dttl#27, sload#28, dload#29, sloss#30, dloss#31, sinpkt#32, dinpkt#33, sjit#34, djit#35, swin#36, stcpb#37L, ... 26 more fields]\n   +- Project [Pred_Label#1019, features_all#1104, id#16, dur#17, proto#18, service#19, state#20, spkts#21, dpkts#22, sbytes#23, dbytes#24, rate#25, sttl#26, dttl#27, sload#28, dload#29, sloss#30, dloss#31, sinpkt#32, dinpkt#33, sjit#34, djit#35, swin#36, stcpb#37L, ... 25 more fields]\n      +- Project [Pred_Label#1019, features_all#1104, id#16, dur#17, proto#18, service#19, state#20, spkts#21, dpkts#22, sbytes#23, dbytes#24, rate#25, sttl#26, dttl#27, sload#28, dload#29, sloss#30, dloss#31, sinpkt#32, dinpkt#33, sjit#34, djit#35, swin#36, stcpb#37L, ... 24 more fields]\n         +- Sample 0.6805, 1.0, false, 99999999\n            +- Sort [Pred_Label#1019 ASC NULLS FIRST, features_all#1104 ASC NULLS FIRST, id#16 ASC NULLS FIRST, dur#17 ASC NULLS FIRST, proto#18 ASC NULLS FIRST, service#19 ASC NULLS FIRST, state#20 ASC NULLS FIRST, spkts#21 ASC NULLS FIRST, dpkts#22 ASC NULLS FIRST, sbytes#23 ASC NULLS FIRST, dbytes#24 ASC NULLS FIRST, rate#25 ASC NULLS FIRST, sttl#26 ASC NULLS FIRST, dttl#27 ASC NULLS FIRST, sload#28 ASC NULLS FIRST, dload#29 ASC NULLS FIRST, sloss#30 ASC NULLS FIRST, dloss#31 ASC NULLS FIRST, sinpkt#32 ASC NULLS FIRST, dinpkt#33 ASC NULLS FIRST, sjit#34 ASC NULLS FIRST, djit#35 ASC NULLS FIRST, swin#36 ASC NULLS FIRST, stcpb#37L ASC NULLS FIRST, ... 23 more fields], false\n               +- Project [Pred_Label#1019, features_all#1104, id#16, dur#17, proto#18, service#19, state#20, spkts#21, dpkts#22, sbytes#23, dbytes#24, rate#25, sttl#26, dttl#27, sload#28, dload#29, sloss#30, dloss#31, sinpkt#32, dinpkt#33, sjit#34, djit#35, swin#36, stcpb#37L, ... 23 more fields]\n                  +- Project [id#16, dur#17, proto#18, service#19, state#20, spkts#21, dpkts#22, sbytes#23, dbytes#24, rate#25, sttl#26, dttl#27, sload#28, dload#29, sloss#30, dloss#31, sinpkt#32, dinpkt#33, sjit#34, djit#35, swin#36, stcpb#37L, dtcpb#38L, dwin#39, ... 29 more fields]\n                     +- Project [id#16, dur#17, proto#18, service#19, state#20, spkts#21, dpkts#22, sbytes#23, dbytes#24, rate#25, sttl#26, dttl#27, sload#28, dload#29, sloss#30, dloss#31, sinpkt#32, dinpkt#33, sjit#34, djit#35, swin#36, stcpb#37L, dtcpb#38L, dwin#39, ... 28 more fields]\n                        +- Project [id#16, dur#17, proto#18, service#19, state#20, spkts#21, dpkts#22, sbytes#23, dbytes#24, rate#25, sttl#26, dttl#27, sload#28, dload#29, sloss#30, dloss#31, sinpkt#32, dinpkt#33, sjit#34, djit#35, swin#36, stcpb#37L, dtcpb#38L, dwin#39, ... 27 more fields]\n                           +- Project [id#16, dur#17, proto#18, service#19, state#20, spkts#21, dpkts#22, sbytes#23, dbytes#24, rate#25, sttl#26, dttl#27, sload#28, dload#29, sloss#30, dloss#31, sinpkt#32, dinpkt#33, sjit#34, djit#35, swin#36, stcpb#37L, dtcpb#38L, dwin#39, ... 26 more fields]\n                              +- Project [id#16, dur#17, proto#18, service#19, state#20, spkts#21, dpkts#22, sbytes#23, dbytes#24, rate#25, sttl#26, dttl#27, sload#28, dload#29, sloss#30, dloss#31, sinpkt#32, dinpkt#33, sjit#34, djit#35, swin#36, stcpb#37L, dtcpb#38L, dwin#39, ... 25 more fields]\n                                 +- Project [id#16, dur#17, proto#18, service#19, state#20, spkts#21, dpkts#22, sbytes#23, dbytes#24, rate#25, sttl#26, dttl#27, sload#28, dload#29, sloss#30, dloss#31, sinpkt#32, dinpkt#33, sjit#34, djit#35, swin#36, stcpb#37L, dtcpb#38L, dwin#39, ... 24 more fields]\n                                    +- Project [id#16, dur#17, proto#18, service#19, state#20, spkts#21, dpkts#22, sbytes#23, dbytes#24, rate#25, sttl#26, dttl#27, sload#28, dload#29, sloss#30, dloss#31, sinpkt#32, dinpkt#33, sjit#34, djit#35, swin#36, stcpb#37L, dtcpb#38L, dwin#39, ... 23 more fields]\n                                       +- Project [id#16, dur#17, proto#18, service#19, state#20, spkts#21, dpkts#22, sbytes#23, dbytes#24, rate#25, sttl#26, dttl#27, sload#28, dload#29, sloss#30, dloss#31, sinpkt#32, dinpkt#33, sjit#34, djit#35, swin#36, stcpb#37L, dtcpb#38L, dwin#39, ... 22 more fields]\n                                          +- Relation [id#16,dur#17,proto#18,service#19,state#20,spkts#21,dpkts#22,sbytes#23,dbytes#24,rate#25,sttl#26,dttl#27,sload#28,dload#29,sloss#30,dloss#31,sinpkt#32,dinpkt#33,sjit#34,djit#35,swin#36,stcpb#37L,dtcpb#38L,dwin#39,... 21 more fields] csv\n"
     ]
    }
   ],
   "source": [
    "# #PRINT CONFUSION MATRIX\n",
    "#Cm=f_predictions.select(\"PoutLabel\",\"label\").distinct().toPandas()\n",
    "#f_predictions.groupBy(\"PoutLabel\",\"prediction\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b369b54a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to train:\n",
      "536.9380197525024\n",
      "Time to predict:\n",
      "3.258512020111084\n"
     ]
    }
   ],
   "source": [
    "print(\"Time to train:\")\n",
    "print(end-start)\n",
    "print(\"Time to predict:\")\n",
    "print(end1-start1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7a50a1c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8473149053503936\n"
     ]
    }
   ],
   "source": [
    "evaluu = BinaryClassificationEvaluator(labelCol=\"Pred_Label\",rawPredictionCol=\"prediction\")\n",
    "print(\"Accuracy of model\")\n",
    "print(evaluu.evaluate(f_predictions))\n",
    "# print(evaluu.explainParams())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3a01b0df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 46:====================================>                     (5 + 3) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY :\n",
      "0.7581129405465359\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# #MULT CLASS\n",
    "evaluator =MulticlassClassificationEvaluator(labelCol=\"Pred_Label\",predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(f_predictions)\n",
    "print(\"ACCURACY :\")\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1f50fcfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score\n",
      "0.6830016540293715\n"
     ]
    }
   ],
   "source": [
    "evaluator =MulticlassClassificationEvaluator(labelCol=\"Pred_Label\",predictionCol=\"prediction\", metricName=\"f1\")\n",
    "score= evaluator.evaluate(f_predictions)\n",
    "print(\"F1-score\")\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bdbb127d",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_yog.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b19242",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
