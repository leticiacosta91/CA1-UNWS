{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a71d16e",
   "metadata": {},
   "source": [
    "# UNSW-NB15: Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "861b9aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  # for array\n",
    "import pandas as pd  # for csv files and dataframe\n",
    "import matplotlib.pyplot as plt  # for plotting\n",
    "import seaborn as sns  # plotting\n",
    "from scipy import stats\n",
    "\n",
    "import pickle  # To load data int disk\n",
    "from prettytable import PrettyTable  # To print in tabular format\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, make_scorer\n",
    "from sklearn.metrics import auc, f1_score, roc_curve\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.model_selection import cross_validate, cross_val_predict\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39c98219",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading data from disk\n",
    "train = pd.read_csv('./train_alldata_EDA.csv')\n",
    "test = pd.read_csv('./test_alldata_EDA.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd1c1315",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function\n",
    "def multi_corr(col1, col2=\"label\", df=train):\n",
    "    '''\n",
    "    This function returns correlation between 2 given features.\n",
    "    Also gives corr of the given features with \"label\" afetr applying log1p to it.\n",
    "    '''\n",
    "    corr = df[[col1, col2]].corr().iloc[0,1]\n",
    "    log_corr = df[col1].apply(np.log1p).corr(df[col2])\n",
    "\n",
    "    print(\"Correlation : {}\\nlog_Correlation: {}\".format(corr, log_corr))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21987f3f",
   "metadata": {},
   "source": [
    "## Removing highly correlated features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15965e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting all the features with high correlation values with other features\n",
    "# Refer: https://chrisalbon.com/machine_learning/feature_selection/drop_highly_correlated_features/\n",
    "corr_matrix = train.corr().abs()\n",
    "\n",
    "# Select upper triangle of correlation matrix\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "\n",
    "# Find index of feature columns with correlation greater than 0.9\n",
    "to_drop = [column for column in upper.columns if any(upper[column] > 0.95)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "66eb2f86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sloss', 'dloss', 'dpkts', 'dwin', 'ltime', 'ct_srv_dst', 'ct_src_dport_ltm', 'ct_dst_src_ltm']\n"
     ]
    }
   ],
   "source": [
    "# We don't want to use these features for plotting because these are having high corr\n",
    "# And most likely have same kind of plots with already plotted feature\n",
    "print(to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c5ded424",
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e3f6b19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_dict['corr_col'] = to_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b9f21ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing the features from train and test data\n",
    "train.drop(columns=to_drop, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f11b5906",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1778032, 41), (762015, 49))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2053deae",
   "metadata": {},
   "source": [
    "## Adding New Features\n",
    "Refer: https://www.elastic.co/guide/en/ecs/master/ecs-network.html\n",
    "\n",
    "Network bytes: Total bytes trasferred by the network. It is sum of 'sbytes' (Source to destination bytes) and 'dbytes' (Destination to source bytes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b8698c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating new features\n",
    "train['network_bytes'] = train['sbytes'] + train['dbytes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1015dbb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1778032, 42), (762015, 49))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1ba10a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping columns which are not useful for the classification\n",
    "# attack_cat is for multiclass classification\n",
    "# all the other columns are address related and not present in sample train data\n",
    "train.drop(['srcip', 'sport', 'dstip', 'dsport', 'attack_cat'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0b28e286",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To use during test data transformation\n",
    "saved_dict['to_drop'] = ['srcip', 'sport', 'dstip', 'dsport', 'attack_cat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ab1eaac8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1778032, 37), (762015, 49))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10eeef5",
   "metadata": {},
   "source": [
    "## Applying log1p on Numerical columns\n",
    "During EDA we found that few numerical columns shows better visualization for pdf curves if we apply log1p to the columns.\n",
    "\n",
    "So I thought to try log1p on all the columns and check the correlation value of the original column and log1p column with target column i.e. \"label\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ad4e927f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting number of unique values of all the columns\n",
    "# If the unique values are high that means it has continuous set of values\n",
    "col_unique_values = train.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "689772fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the unique values are getter than some threshould than we will check its corr\n",
    "col = col_unique_values[col_unique_values>200].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "49a2fd87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------dur--------------\n",
      "Correlation : 0.0019274028701131475\n",
      "log_Correlation: -0.03254413756460629\n",
      "------------sbytes------------\n",
      "Correlation : 0.010344749695229565\n",
      "log_Correlation: -0.356163155589847\n",
      "------------dbytes------------\n",
      "Correlation : -0.07641408324436148\n",
      "log_Correlation: -0.5193868283741494\n",
      "------------sload-------------\n",
      "Correlation : 0.19211948100086756\n",
      "log_Correlation: 0.3474660145034936\n",
      "------------dload-------------\n",
      "Correlation : -0.21978094390126515\n",
      "log_Correlation: -0.6033545881626396\n",
      "------------spkts-------------\n",
      "Correlation : -0.12200425437154418\n",
      "log_Correlation: -0.31635338269675894\n",
      "------------stcpb-------------\n",
      "Correlation : -0.23365153315010911\n",
      "log_Correlation: -0.3135563222142901\n",
      "------------dtcpb-------------\n",
      "Correlation : -0.23346071773809843\n",
      "log_Correlation: -0.31340064798121137\n",
      "-----------smeansz------------\n",
      "Correlation : -0.06517990378993671\n",
      "log_Correlation: -0.15111450989648387\n",
      "-----------dmeansz------------\n",
      "Correlation : -0.27230605607442226\n",
      "log_Correlation: -0.5646402127401746\n",
      "---------res_bdy_len----------\n",
      "Correlation : -0.0268968526601601\n",
      "log_Correlation: -0.06794901821505824\n",
      "-------------sjit-------------\n",
      "Correlation : 0.020974632769798898\n",
      "log_Correlation: -0.1893910004654401\n",
      "-------------djit-------------\n",
      "Correlation : -0.05608646071436526\n",
      "log_Correlation: -0.2048707569412939\n",
      "------------stime-------------\n",
      "Correlation : 0.27571439334451364\n",
      "log_Correlation: 0.2757139750761008\n",
      "-----------sintpkt------------\n",
      "Correlation : -0.018971416506195472\n",
      "log_Correlation: -0.08027556633947532\n",
      "-----------dintpkt------------\n",
      "Correlation : -0.010747058729631331\n",
      "log_Correlation: -0.07938735519639521\n",
      "------------tcprtt------------\n",
      "Correlation : 0.14592508147519564\n",
      "log_Correlation: 0.18606697323316698\n",
      "------------synack------------\n",
      "Correlation : 0.12439627235187588\n",
      "log_Correlation: 0.15522598836025045\n",
      "------------ackdat------------\n",
      "Correlation : 0.14712408841119426\n",
      "log_Correlation: 0.1796204080402249\n",
      "--------network_bytes---------\n",
      "Correlation : -0.06790965631742822\n",
      "log_Correlation: -0.41043328745985647\n"
     ]
    }
   ],
   "source": [
    "# Checking corr value of original col and log1p applied col\n",
    "# Taking those columns whose unique values are getter than some threshould\n",
    "for column in col:\n",
    "    print(\"{:-^30}\".format(column))\n",
    "    multi_corr(column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9dd8e569",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Will apply log1p on this columns and remove original columns\n",
    "log1p_col = ['dur', 'sbytes', 'dbytes', 'sload', 'dload', 'spkts', 'stcpb', 'dtcpb', 'smeansz', 'dmeansz', 'sjit', 'djit', 'network_bytes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "363b48e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_dict['log1p_col'] = log1p_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5bd5fc37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mode values of every features, will use to fill Null values of test\n",
    "mode_dict = train.mode().iloc[0].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3dfe86a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log1p_transform(col, df=train):\n",
    "    '''\n",
    "    Apply log1p on given column.\n",
    "    Remove the original cola and keep log1p applied col\n",
    "    '''\n",
    "    new_col = col+'_log1p'\n",
    "    df[new_col] = df[col].apply(np.log1p)\n",
    "    df.drop(col, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e168fdbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforming columns with log1p\n",
    "for col in log1p_col:\n",
    "    log1p_transform(col, df=train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1e20a9bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1778032, 37)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7853935a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['proto', 'state', 'sttl', 'dttl', 'service', 'swin', 'trans_depth',\n",
       "       'res_bdy_len', 'stime', 'sintpkt', 'dintpkt', 'tcprtt', 'synack',\n",
       "       'ackdat', 'is_sm_ips_ports', 'ct_state_ttl', 'ct_flw_http_mthd',\n",
       "       'is_ftp_login', 'ct_ftp_cmd', 'ct_srv_src', 'ct_dst_ltm', 'ct_src_ltm',\n",
       "       'ct_dst_sport_ltm', 'label', 'dur_log1p', 'sbytes_log1p',\n",
       "       'dbytes_log1p', 'sload_log1p', 'dload_log1p', 'spkts_log1p',\n",
       "       'stcpb_log1p', 'dtcpb_log1p', 'smeansz_log1p', 'dmeansz_log1p',\n",
       "       'sjit_log1p', 'djit_log1p', 'network_bytes_log1p'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c8ef6b46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1778032, 37), (762015, 49))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b0c2af39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating x and y set from the dataset\n",
    "x_train, y_train = train.drop(columns=['label']), train['label']\n",
    "x_test, y_test = test.drop(columns=['label']), test['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e37119cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1778032, 36) (1778032,)\n",
      "\n",
      "(762015, 48) (762015,)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape, y_train.shape)\n",
    "print()\n",
    "print(x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "eb065df3",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '.final_ipynb/final_train.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [41]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Saving all the files to disk to use later\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m pickle\u001b[38;5;241m.\u001b[39mdump((x_train, y_train), \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m.final_ipynb/final_train.pkl\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mwb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[0;32m      3\u001b[0m pickle\u001b[38;5;241m.\u001b[39mdump((x_test, y_test), \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.final_ipynb/final_test.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '.final_ipynb/final_train.pkl'"
     ]
    }
   ],
   "source": [
    "# Saving all the files to disk to use later\n",
    "pickle.dump((x_train, y_train), open('.final_ipynb/final_train.pkl', 'wb'))\n",
    "pickle.dump((x_test, y_test), open('.final_ipynb/final_test.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9b70d7d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivos salvos com sucesso.\n"
     ]
    }
   ],
   "source": [
    "# Saving all the files to disk to use later\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "\n",
    "caminho_arquivo_train = os.path.join(caminho_absoluto_final_ipynb, 'final_train.pkl')\n",
    "caminho_arquivo_test = os.path.join(caminho_absoluto_final_ipynb, 'final_test.pkl')\n",
    "\n",
    "# Salvando os arquivos\n",
    "with open(caminho_arquivo_train, 'wb') as f:\n",
    "    pickle.dump((x_train, y_train), f)\n",
    "with open(caminho_arquivo_test, 'wb') as f:\n",
    "    pickle.dump((x_test, y_test), f)\n",
    "\n",
    "print(\"Arquivos salvos com sucesso.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1d98c90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting categorical and numerical columns in 2 diff lists\n",
    "cat_col = ['proto', 'service', 'state']\n",
    "num_col = list(set(x_train.columns) - set(cat_col))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "853fefff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To use later, during test data cleaning\n",
    "saved_dict['cat_col'] = cat_col\n",
    "saved_dict['num_col'] = num_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b69a949d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>proto</th>\n",
       "      <th>state</th>\n",
       "      <th>sttl</th>\n",
       "      <th>dttl</th>\n",
       "      <th>service</th>\n",
       "      <th>swin</th>\n",
       "      <th>trans_depth</th>\n",
       "      <th>res_bdy_len</th>\n",
       "      <th>stime</th>\n",
       "      <th>sintpkt</th>\n",
       "      <th>...</th>\n",
       "      <th>sload_log1p</th>\n",
       "      <th>dload_log1p</th>\n",
       "      <th>spkts_log1p</th>\n",
       "      <th>stcpb_log1p</th>\n",
       "      <th>dtcpb_log1p</th>\n",
       "      <th>smeansz_log1p</th>\n",
       "      <th>dmeansz_log1p</th>\n",
       "      <th>sjit_log1p</th>\n",
       "      <th>djit_log1p</th>\n",
       "      <th>network_bytes_log1p</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>udp</td>\n",
       "      <td>INT</td>\n",
       "      <td>254</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1421930643</td>\n",
       "      <td>33.479000</td>\n",
       "      <td>...</td>\n",
       "      <td>9.275600</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.609438</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.828641</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.878042</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.187386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>udp</td>\n",
       "      <td>INT</td>\n",
       "      <td>60</td>\n",
       "      <td>0</td>\n",
       "      <td>dns</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1424246229</td>\n",
       "      <td>0.008000</td>\n",
       "      <td>...</td>\n",
       "      <td>18.698312</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.098612</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.890349</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.579730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tcp</td>\n",
       "      <td>FIN</td>\n",
       "      <td>31</td>\n",
       "      <td>29</td>\n",
       "      <td>None</td>\n",
       "      <td>255</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1421948071</td>\n",
       "      <td>0.372205</td>\n",
       "      <td>...</td>\n",
       "      <td>14.105347</td>\n",
       "      <td>16.314201</td>\n",
       "      <td>3.713572</td>\n",
       "      <td>20.196135</td>\n",
       "      <td>21.733479</td>\n",
       "      <td>4.174387</td>\n",
       "      <td>6.313548</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.012070</td>\n",
       "      <td>10.152883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tcp</td>\n",
       "      <td>FIN</td>\n",
       "      <td>31</td>\n",
       "      <td>29</td>\n",
       "      <td>ftp</td>\n",
       "      <td>255</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1421971944</td>\n",
       "      <td>16.144740</td>\n",
       "      <td>...</td>\n",
       "      <td>10.258074</td>\n",
       "      <td>10.501435</td>\n",
       "      <td>3.970292</td>\n",
       "      <td>21.803017</td>\n",
       "      <td>20.494420</td>\n",
       "      <td>4.043051</td>\n",
       "      <td>4.248495</td>\n",
       "      <td>7.264606</td>\n",
       "      <td>3.984562</td>\n",
       "      <td>8.806124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tcp</td>\n",
       "      <td>FIN</td>\n",
       "      <td>31</td>\n",
       "      <td>29</td>\n",
       "      <td>None</td>\n",
       "      <td>255</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1421963050</td>\n",
       "      <td>1.218800</td>\n",
       "      <td>...</td>\n",
       "      <td>13.339317</td>\n",
       "      <td>13.412088</td>\n",
       "      <td>2.833213</td>\n",
       "      <td>20.673269</td>\n",
       "      <td>21.855078</td>\n",
       "      <td>4.574711</td>\n",
       "      <td>4.521789</td>\n",
       "      <td>4.309533</td>\n",
       "      <td>1.138118</td>\n",
       "      <td>8.066208</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  proto state  sttl  dttl service  swin  trans_depth  res_bdy_len       stime  \\\n",
       "0   udp   INT   254     0    None     0            0            0  1421930643   \n",
       "1   udp   INT    60     0     dns     0            0            0  1424246229   \n",
       "2   tcp   FIN    31    29    None   255            0            0  1421948071   \n",
       "3   tcp   FIN    31    29     ftp   255            0            0  1421971944   \n",
       "4   tcp   FIN    31    29    None   255            0            0  1421963050   \n",
       "\n",
       "     sintpkt  ...  sload_log1p  dload_log1p  spkts_log1p  stcpb_log1p  \\\n",
       "0  33.479000  ...     9.275600     0.000000     1.609438     0.000000   \n",
       "1   0.008000  ...    18.698312     0.000000     1.098612     0.000000   \n",
       "2   0.372205  ...    14.105347    16.314201     3.713572    20.196135   \n",
       "3  16.144740  ...    10.258074    10.501435     3.970292    21.803017   \n",
       "4   1.218800  ...    13.339317    13.412088     2.833213    20.673269   \n",
       "\n",
       "   dtcpb_log1p  smeansz_log1p  dmeansz_log1p  sjit_log1p  djit_log1p  \\\n",
       "0     0.000000       3.828641       0.000000    3.878042    0.000000   \n",
       "1     0.000000       4.890349       0.000000    0.000000    0.000000   \n",
       "2    21.733479       4.174387       6.313548    0.000000    3.012070   \n",
       "3    20.494420       4.043051       4.248495    7.264606    3.984562   \n",
       "4    21.855078       4.574711       4.521789    4.309533    1.138118   \n",
       "\n",
       "   network_bytes_log1p  \n",
       "0             5.187386  \n",
       "1             5.579730  \n",
       "2            10.152883  \n",
       "3             8.806124  \n",
       "4             8.066208  \n",
       "\n",
       "[5 rows x 36 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a35ea5",
   "metadata": {},
   "source": [
    "## Standardizing\n",
    "As we have seen that the range of few features in this dataset is very large. So we will keep everything within certain range by applying standardscaler. After this all the features will have mean 0 and std 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d52df107",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardizing the data\n",
    "scaler = StandardScaler()\n",
    "scaler = scaler.fit(x_train[num_col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "536efed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train[num_col] = scaler.transform(x_train[num_col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "bf52c149",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>proto</th>\n",
       "      <th>state</th>\n",
       "      <th>sttl</th>\n",
       "      <th>dttl</th>\n",
       "      <th>service</th>\n",
       "      <th>swin</th>\n",
       "      <th>trans_depth</th>\n",
       "      <th>res_bdy_len</th>\n",
       "      <th>stime</th>\n",
       "      <th>sintpkt</th>\n",
       "      <th>...</th>\n",
       "      <th>sload_log1p</th>\n",
       "      <th>dload_log1p</th>\n",
       "      <th>spkts_log1p</th>\n",
       "      <th>stcpb_log1p</th>\n",
       "      <th>dtcpb_log1p</th>\n",
       "      <th>smeansz_log1p</th>\n",
       "      <th>dmeansz_log1p</th>\n",
       "      <th>sjit_log1p</th>\n",
       "      <th>djit_log1p</th>\n",
       "      <th>network_bytes_log1p</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>udp</td>\n",
       "      <td>INT</td>\n",
       "      <td>2.561444</td>\n",
       "      <td>-0.717760</td>\n",
       "      <td>None</td>\n",
       "      <td>-1.196045</td>\n",
       "      <td>-0.225343</td>\n",
       "      <td>-0.089113</td>\n",
       "      <td>-1.172764</td>\n",
       "      <td>-0.057567</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.383776</td>\n",
       "      <td>-1.879995</td>\n",
       "      <td>-0.704801</td>\n",
       "      <td>-1.190007</td>\n",
       "      <td>-1.189697</td>\n",
       "      <td>-1.137341</td>\n",
       "      <td>-1.850553</td>\n",
       "      <td>0.204270</td>\n",
       "      <td>-0.899657</td>\n",
       "      <td>-1.229918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>udp</td>\n",
       "      <td>INT</td>\n",
       "      <td>-0.037542</td>\n",
       "      <td>-0.717760</td>\n",
       "      <td>dns</td>\n",
       "      <td>-1.196045</td>\n",
       "      <td>-0.225343</td>\n",
       "      <td>-0.089113</td>\n",
       "      <td>0.868469</td>\n",
       "      <td>-0.069616</td>\n",
       "      <td>...</td>\n",
       "      <td>1.614925</td>\n",
       "      <td>-1.879995</td>\n",
       "      <td>-1.080734</td>\n",
       "      <td>-1.190007</td>\n",
       "      <td>-1.189697</td>\n",
       "      <td>0.565988</td>\n",
       "      <td>-1.850553</td>\n",
       "      <td>-0.995343</td>\n",
       "      <td>-0.899657</td>\n",
       "      <td>-1.062092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tcp</td>\n",
       "      <td>FIN</td>\n",
       "      <td>-0.426051</td>\n",
       "      <td>-0.041365</td>\n",
       "      <td>None</td>\n",
       "      <td>0.836096</td>\n",
       "      <td>-0.225343</td>\n",
       "      <td>-0.089113</td>\n",
       "      <td>-1.157401</td>\n",
       "      <td>-0.069485</td>\n",
       "      <td>...</td>\n",
       "      <td>0.153252</td>\n",
       "      <td>0.988351</td>\n",
       "      <td>0.843701</td>\n",
       "      <td>0.741629</td>\n",
       "      <td>0.888925</td>\n",
       "      <td>-0.582651</td>\n",
       "      <td>0.881301</td>\n",
       "      <td>-0.995343</td>\n",
       "      <td>0.170283</td>\n",
       "      <td>0.894089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tcp</td>\n",
       "      <td>FIN</td>\n",
       "      <td>-0.426051</td>\n",
       "      <td>-0.041365</td>\n",
       "      <td>ftp</td>\n",
       "      <td>0.836096</td>\n",
       "      <td>-0.225343</td>\n",
       "      <td>-0.089113</td>\n",
       "      <td>-1.136357</td>\n",
       "      <td>-0.063807</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.071111</td>\n",
       "      <td>-0.033644</td>\n",
       "      <td>1.032630</td>\n",
       "      <td>0.895318</td>\n",
       "      <td>0.770420</td>\n",
       "      <td>-0.793357</td>\n",
       "      <td>-0.012242</td>\n",
       "      <td>1.251851</td>\n",
       "      <td>0.515730</td>\n",
       "      <td>0.318009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tcp</td>\n",
       "      <td>FIN</td>\n",
       "      <td>-0.426051</td>\n",
       "      <td>-0.041365</td>\n",
       "      <td>None</td>\n",
       "      <td>0.836096</td>\n",
       "      <td>-0.225343</td>\n",
       "      <td>-0.089113</td>\n",
       "      <td>-1.144197</td>\n",
       "      <td>-0.069180</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.090531</td>\n",
       "      <td>0.478104</td>\n",
       "      <td>0.195816</td>\n",
       "      <td>0.787264</td>\n",
       "      <td>0.900555</td>\n",
       "      <td>0.059601</td>\n",
       "      <td>0.106012</td>\n",
       "      <td>0.337745</td>\n",
       "      <td>-0.495378</td>\n",
       "      <td>0.001507</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  proto state      sttl      dttl service      swin  trans_depth  res_bdy_len  \\\n",
       "0   udp   INT  2.561444 -0.717760    None -1.196045    -0.225343    -0.089113   \n",
       "1   udp   INT -0.037542 -0.717760     dns -1.196045    -0.225343    -0.089113   \n",
       "2   tcp   FIN -0.426051 -0.041365    None  0.836096    -0.225343    -0.089113   \n",
       "3   tcp   FIN -0.426051 -0.041365     ftp  0.836096    -0.225343    -0.089113   \n",
       "4   tcp   FIN -0.426051 -0.041365    None  0.836096    -0.225343    -0.089113   \n",
       "\n",
       "      stime   sintpkt  ...  sload_log1p  dload_log1p  spkts_log1p  \\\n",
       "0 -1.172764 -0.057567  ...    -1.383776    -1.879995    -0.704801   \n",
       "1  0.868469 -0.069616  ...     1.614925    -1.879995    -1.080734   \n",
       "2 -1.157401 -0.069485  ...     0.153252     0.988351     0.843701   \n",
       "3 -1.136357 -0.063807  ...    -1.071111    -0.033644     1.032630   \n",
       "4 -1.144197 -0.069180  ...    -0.090531     0.478104     0.195816   \n",
       "\n",
       "   stcpb_log1p  dtcpb_log1p  smeansz_log1p  dmeansz_log1p  sjit_log1p  \\\n",
       "0    -1.190007    -1.189697      -1.137341      -1.850553    0.204270   \n",
       "1    -1.190007    -1.189697       0.565988      -1.850553   -0.995343   \n",
       "2     0.741629     0.888925      -0.582651       0.881301   -0.995343   \n",
       "3     0.895318     0.770420      -0.793357      -0.012242    1.251851   \n",
       "4     0.787264     0.900555       0.059601       0.106012    0.337745   \n",
       "\n",
       "   djit_log1p  network_bytes_log1p  \n",
       "0   -0.899657            -1.229918  \n",
       "1   -0.899657            -1.062092  \n",
       "2    0.170283             0.894089  \n",
       "3    0.515730             0.318009  \n",
       "4   -0.495378             0.001507  \n",
       "\n",
       "[5 rows x 36 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c4023e",
   "metadata": {},
   "source": [
    "## Onehot Encoding\n",
    "In our dataset we have few categorical columns with text data. But ML models can't process text data it can process numbers.\n",
    "\n",
    "So we have to convert categorical columns to numerical columns in some way. We will use onehotencoder where we will assign 1 if the value is present for the row and rest of the columns will be 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "73906c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Onehot Encoding\n",
    "service_ = OneHotEncoder()\n",
    "proto_ = OneHotEncoder()\n",
    "state_ = OneHotEncoder()\n",
    "ohe_service = service_.fit(x_train.service.values.reshape(-1,1))\n",
    "ohe_proto = proto_.fit(x_train.proto.values.reshape(-1,1))\n",
    "ohe_state = state_.fit(x_train.state.values.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "912acee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are onehot encoding the given column\n",
    "# Remove the original categorical column\n",
    "for col, ohe in zip(['proto', 'service', 'state'], [ohe_proto, ohe_service, ohe_state]):\n",
    "    x = ohe.transform(x_train[col].values.reshape(-1,1))\n",
    "    tmp_df = pd.DataFrame(x.todense(), columns=[col+'_'+i for i in ohe.categories_[0]])\n",
    "    x_train = pd.concat([x_train.drop(col, axis=1), tmp_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a9b0e104",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sttl</th>\n",
       "      <th>dttl</th>\n",
       "      <th>swin</th>\n",
       "      <th>trans_depth</th>\n",
       "      <th>res_bdy_len</th>\n",
       "      <th>stime</th>\n",
       "      <th>sintpkt</th>\n",
       "      <th>dintpkt</th>\n",
       "      <th>tcprtt</th>\n",
       "      <th>synack</th>\n",
       "      <th>...</th>\n",
       "      <th>state_INT</th>\n",
       "      <th>state_MAS</th>\n",
       "      <th>state_PAR</th>\n",
       "      <th>state_REQ</th>\n",
       "      <th>state_RST</th>\n",
       "      <th>state_TST</th>\n",
       "      <th>state_TXD</th>\n",
       "      <th>state_URH</th>\n",
       "      <th>state_URN</th>\n",
       "      <th>state_no</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.561444</td>\n",
       "      <td>-0.717760</td>\n",
       "      <td>-1.196045</td>\n",
       "      <td>-0.225343</td>\n",
       "      <td>-0.089113</td>\n",
       "      <td>-1.172764</td>\n",
       "      <td>-0.057567</td>\n",
       "      <td>-0.055099</td>\n",
       "      <td>-0.136439</td>\n",
       "      <td>-0.128893</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.037542</td>\n",
       "      <td>-0.717760</td>\n",
       "      <td>-1.196045</td>\n",
       "      <td>-0.225343</td>\n",
       "      <td>-0.089113</td>\n",
       "      <td>0.868469</td>\n",
       "      <td>-0.069616</td>\n",
       "      <td>-0.055099</td>\n",
       "      <td>-0.136439</td>\n",
       "      <td>-0.128893</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.426051</td>\n",
       "      <td>-0.041365</td>\n",
       "      <td>0.836096</td>\n",
       "      <td>-0.225343</td>\n",
       "      <td>-0.089113</td>\n",
       "      <td>-1.157401</td>\n",
       "      <td>-0.069485</td>\n",
       "      <td>-0.054857</td>\n",
       "      <td>-0.121383</td>\n",
       "      <td>-0.107394</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.426051</td>\n",
       "      <td>-0.041365</td>\n",
       "      <td>0.836096</td>\n",
       "      <td>-0.225343</td>\n",
       "      <td>-0.089113</td>\n",
       "      <td>-1.136357</td>\n",
       "      <td>-0.063807</td>\n",
       "      <td>-0.044512</td>\n",
       "      <td>-0.120830</td>\n",
       "      <td>-0.107159</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.426051</td>\n",
       "      <td>-0.041365</td>\n",
       "      <td>0.836096</td>\n",
       "      <td>-0.225343</td>\n",
       "      <td>-0.089113</td>\n",
       "      <td>-1.144197</td>\n",
       "      <td>-0.069180</td>\n",
       "      <td>-0.054358</td>\n",
       "      <td>-0.123351</td>\n",
       "      <td>-0.110421</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 197 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       sttl      dttl      swin  trans_depth  res_bdy_len     stime   sintpkt  \\\n",
       "0  2.561444 -0.717760 -1.196045    -0.225343    -0.089113 -1.172764 -0.057567   \n",
       "1 -0.037542 -0.717760 -1.196045    -0.225343    -0.089113  0.868469 -0.069616   \n",
       "2 -0.426051 -0.041365  0.836096    -0.225343    -0.089113 -1.157401 -0.069485   \n",
       "3 -0.426051 -0.041365  0.836096    -0.225343    -0.089113 -1.136357 -0.063807   \n",
       "4 -0.426051 -0.041365  0.836096    -0.225343    -0.089113 -1.144197 -0.069180   \n",
       "\n",
       "    dintpkt    tcprtt    synack  ...  state_INT  state_MAS  state_PAR  \\\n",
       "0 -0.055099 -0.136439 -0.128893  ...        1.0        0.0        0.0   \n",
       "1 -0.055099 -0.136439 -0.128893  ...        1.0        0.0        0.0   \n",
       "2 -0.054857 -0.121383 -0.107394  ...        0.0        0.0        0.0   \n",
       "3 -0.044512 -0.120830 -0.107159  ...        0.0        0.0        0.0   \n",
       "4 -0.054358 -0.123351 -0.110421  ...        0.0        0.0        0.0   \n",
       "\n",
       "   state_REQ  state_RST  state_TST  state_TXD  state_URH  state_URN  state_no  \n",
       "0        0.0        0.0        0.0        0.0        0.0        0.0       0.0  \n",
       "1        0.0        0.0        0.0        0.0        0.0        0.0       0.0  \n",
       "2        0.0        0.0        0.0        0.0        0.0        0.0       0.0  \n",
       "3        0.0        0.0        0.0        0.0        0.0        0.0       0.0  \n",
       "4        0.0        0.0        0.0        0.0        0.0        0.0       0.0  \n",
       "\n",
       "[5 rows x 197 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c5cd041e",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'final_ipynb/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1cf1e2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(scaler, open(file_path+'scaler.pkl', 'wb'))  # Standard scaler\n",
    "pickle.dump(saved_dict, open(file_path+'saved_dict.pkl', 'wb'))  # Dictionary with important parameters\n",
    "pickle.dump(mode_dict, open(file_path+'mode_dict.pkl', 'wb'))  #  Dictionary with most frequent values of columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e6be15aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Onehot encoder for categorical columns\n",
    "pickle.dump(ohe_proto, open(file_path+'ohe_proto.pkl', 'wb'))\n",
    "pickle.dump(ohe_service, open(file_path+'ohe_service.pkl', 'wb'))\n",
    "pickle.dump(ohe_state, open(file_path+'ohe_state.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6acad3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaned and processed train data\n",
    "pickle.dump((x_train, y_train), open(file_path+'final_train.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4732b48",
   "metadata": {},
   "source": [
    "## Pipeline functions\n",
    "We have to prepare a pipeline, where we can send raw data and get the output.\n",
    "\n",
    "We will use test data to implement the pipeline. Here we will use all the parameters we have saved using train data.\n",
    "\n",
    "Also standardize and onehot encode test data using train data objects for standardscaler and onehotencoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5ea1bc4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(data):\n",
    "    '''\n",
    "    Cleans given raw data. Performs various cleaning, removes Null and wrong values.\n",
    "    Check for columns datatype and fix them.\n",
    "    '''\n",
    "    numerical_col = data.select_dtypes(include=np.number).columns  # All the numerical columns list\n",
    "    categorical_col = data.select_dtypes(exclude=np.number).columns  # All the categorical columns list\n",
    "    \n",
    "    # Cleaning the data\n",
    "    for col in data.columns:\n",
    "        val = mode_dict[col]  # Mode value of the column in train data\n",
    "        data[col] = data[col].fillna(value=val)\n",
    "        data[col] = data[col].replace(' ', value=val)\n",
    "        data[col] = data[col].apply(lambda x:\"None\" if x==\"-\" else x)\n",
    "\n",
    "        # Fixing binary columns\n",
    "        if col in saved_dict['binary_col']:\n",
    "            data[col] = np.where(data[col]>1, val, data[col])\n",
    "\n",
    "    # Fixing datatype of columns\n",
    "    bad_dtypes = list(set(categorical_col) - set(saved_dict['cat_col']))\n",
    "    for bad_col in bad_dtypes:\n",
    "        data[col] = data[col].astype(float)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "22e9c45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_log1p(data):\n",
    "    '''\n",
    "    Performs FE on the data. Apply log1p on the specified columns create new column and remove those original columns.\n",
    "    '''\n",
    "    for col in saved_dict['log1p_col']:\n",
    "        new_col = col + '_log1p'  # New col name\n",
    "        data[new_col] = data[col].apply(np.log1p)  # Creating new column on transformed data\n",
    "        data.drop(col, axis=1, inplace=True)  # Removing old columns\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "cf816601",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize(data):\n",
    "    '''\n",
    "    Stanardize the given data. Performs mean centering and varience scaling.\n",
    "    Using stanardscaler object trained on train data.\n",
    "    '''\n",
    "    data[saved_dict['num_col']] = scaler.transform(data[saved_dict['num_col']])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "79d23fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ohencoding(data):\n",
    "    '''\n",
    "    Onehot encoding the categoricla columns.\n",
    "    Add the ohe columns with the data and removes categorical columns.\n",
    "    Using Onehotencoder objects trained on train data.\n",
    "    '''\n",
    "\n",
    "    # Onehot encoding cat col using onehotencoder objects\n",
    "    X = ohe_service.transform(data['service'].values.reshape(-1, 1))\n",
    "    Xm = ohe_proto.transform(data['proto'].values.reshape(-1, 1))\n",
    "    Xmm = ohe_state.transform(data['state'].values.reshape(-1, 1))\n",
    "    \n",
    "    # Adding encoding data to original data\n",
    "    data = pd.concat([data,\n",
    "                      pd.DataFrame(Xm.toarray(), columns=['proto_'+i for i in ohe_proto.categories_[0]]),\n",
    "                      pd.DataFrame(X.toarray(), columns=['service_'+i for i in ohe_service.categories_[0]]),\n",
    "                      pd.DataFrame(Xmm.toarray(), columns=['state_'+i for i in ohe_state.categories_[0]])],\n",
    "                      axis=1)\n",
    "    \n",
    "    # Removing cat columns\n",
    "    data.drop(['proto', 'service', 'state'], axis=1, inplace=True)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "35534763",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parametrs\n",
    "saved_dict = pickle.load(open(file_path+'saved_dict.pkl', 'rb'))\n",
    "# Mode value of all the columns\n",
    "mode_dict = pickle.load(open(file_path+'mode_dict.pkl', 'rb'))\n",
    "# Stanardscaler object\n",
    "scaler = pickle.load(open(file_path+'scaler.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "62673af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hot encoder objects\n",
    "ohe_proto = pickle.load(open(file_path+'ohe_proto.pkl', 'rb'))\n",
    "ohe_service = pickle.load(open(file_path+'ohe_service.pkl', 'rb'))\n",
    "ohe_state = pickle.load(open(file_path+'ohe_state.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "27acefa3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(762015, 48)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f46f03f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resetting index of test data\n",
    "x_test.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "12b07aaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(762015, 48)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "cccc4e18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['srcip', 'sport', 'dstip', 'dsport', 'proto', 'state', 'dur', 'sbytes',\n",
       "       'dbytes', 'sttl', 'dttl', 'sloss', 'dloss', 'service', 'sload', 'dload',\n",
       "       'spkts', 'dpkts', 'swin', 'dwin', 'stcpb', 'dtcpb', 'smeansz',\n",
       "       'dmeansz', 'trans_depth', 'res_bdy_len', 'sjit', 'djit', 'stime',\n",
       "       'ltime', 'sintpkt', 'dintpkt', 'tcprtt', 'synack', 'ackdat',\n",
       "       'is_sm_ips_ports', 'ct_state_ttl', 'ct_flw_http_mthd', 'is_ftp_login',\n",
       "       'ct_ftp_cmd', 'ct_srv_src', 'ct_srv_dst', 'ct_dst_ltm', 'ct_src_ltm',\n",
       "       'ct_src_dport_ltm', 'ct_dst_sport_ltm', 'ct_dst_src_ltm', 'attack_cat'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "0a1b2e69",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'columns'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[1;32mIn [69]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Adding column names\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m x_test\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;241m=\u001b[39m \u001b[43msaved_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcolumns\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'columns'"
     ]
    }
   ],
   "source": [
    "# Adding column names\n",
    "x_test.columns = saved_dict['columns']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "0f011f76",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "EOL while scanning string literal (3188918964.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Input \u001b[1;32mIn [70]\u001b[1;36m\u001b[0m\n\u001b[1;33m    x_test['network_bytes'] = x_test['dbytes'] + x_test['sbytes\u001b[0m\n\u001b[1;37m                                                               ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m EOL while scanning string literal\n"
     ]
    }
   ],
   "source": [
    "# Creating new Feature\n",
    "x_test['network_bytes'] = x_test['dbytes'] + x_test['sbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517c3c13",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
